{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import all necessary packages\n",
    "\n",
    "from urllib2 import urlopen\n",
    "import json\n",
    "import pandas as pd\n",
    "import elasticsearch\n",
    "import requests\n",
    "import indicoio\n",
    "from sklearn.cluster import MeanShift\n",
    "from nltk.corpus import stopwords\n",
    "indicoio.config.api_key = '9369f9b0a0d9333eeb47aa93f44ffe0f'\n",
    "import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import ast\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def cleanstring(x):\n",
    "    re.sub('[^a-zA-Z0-9-_*.]','', x)\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "from HTMLParser import HTMLParser\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "# import stopword\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "stopset.update([\"em\", \"strong\", 'wp', 'com', '8217', \"jpeg\", \"http\", \"uploads\", \"new\", \"span\", \"family\", \"font\", \"arial\", \n",
    "                \"game\", \"style\", \"color\", \"twitter\", \"monothetic\", \"id\", \"div\", \"class\", \"text\", \"input\", \"re\", \"type\", \"src\",\n",
    "               \"value\", \"data\", \"maxlength\", \"levels\", \"facebook\", \"first\", \"test\", \"wmode\", \"ifr_source\", \"h4\", \"attr\", \"function\"\n",
    "               , \"video\", \"img\", \"_blank\", \"https\", \"content\", \"target\", \"href\", \"www\", \"000000\", \"000080\", \"ul li\", \"en us\", \"xbox\",\n",
    "               \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"2015\", \"2016\", \"2017\", \"xbox one\", \"news\", \"alt\", \"srcset\"\n",
    "               , \"youtube\", \"one\", \"jpg\", \"height\", \"en\", \"wire\", \"px\", \"11\", \"pt\", \"xbox wire\", \"xbox live\", \"microsoft\", \"game\"\n",
    "               , \"games\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start an instance of elasticsearch\n",
    "es = elasticsearch.Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "\n",
    "# create the mappings of elasticsearch for the webhose articles\n",
    "\n",
    "mapping = {\n",
    "    \"articles\": {\n",
    "        \"properties\": {\n",
    "            \"article_title\": {\"type\": \"string\", \"similarity\": \"default\"},\n",
    "            \"link\": {\"type\": \"string\"},\n",
    "            \"content\": {\"type\": \"string\", \"similarity\": \"default\"},\n",
    "            \"author\": {\"type\": \"string\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a new \"xbwire\" index that includes \"articles\" with the above mapping\n",
    "es.indices.create(\"xbwire5\")\n",
    "es.indices.put_mapping(index=\"xbwire5\", doc_type=\"articles\", body=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the Xbox Wire articles, break them into the title, link, body, and author \n",
    "\n",
    "# store them within articles_coll\n",
    "\n",
    "articles_coll = []\n",
    "\n",
    "page_num = 1\n",
    "while page_num < 3:\n",
    "    str_pn = str(page_num)\n",
    "    json_url = 'https://news.xbox.com/wp-json/posts' + '?page=' + str_pn\n",
    "    open_url = urlopen(json_url)\n",
    "    data = json.load(open_url)\n",
    "    for x in data:\n",
    "        articles = x['title'].encode('utf-8'), x['link'], strip_tags(x['content'].encode('utf-8').lower()), x['author']['name']\n",
    "        articles_coll.append(articles)\n",
    "    page_num += 1\n",
    "    \n",
    "first_url_string = 'https://webhose.io/search?token=89194856-ed59-4c4b-8037-758625bf7327&format=json&q=xbox%20language%3A(english)%20thread.country%3AUS%20site%3A'\n",
    "end_url_string = '%20(site_type%3Anews%20OR%20site_type%3Ablogs)'\n",
    "link_excel = pd.read_excel('Publication with Links.xlsx')\n",
    "\n",
    "publications = pd.read_excel('Publication with Links.xlsx')\n",
    "\n",
    "urls = ['gamespot.com', 'ign.com', 'kotaku.com', 'theverge.com', 'polygon.com', 'engadget.com', 'gizmodo.com', 'destructoid.com', 'cnet.com']\n",
    "\n",
    "webhose_articles = []\n",
    "\n",
    "# get the webhose articles\n",
    "\n",
    "for x in urls:\n",
    "    r = urlopen(first_url_string + x + end_url_string)\n",
    "    data = json.load(r)\n",
    "    for x in data['posts']:\n",
    "        articles = x['title'].encode('utf-8').lower(), x['url'], strip_tags(x['text'].encode('utf-8').lower()), x['author']\n",
    "        webhose_articles.append(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using Indico.io, make a set of keywords and entities, and pass it back to the articles_coll so that everything travels together\n",
    "\n",
    "new_articles_coll = []\n",
    "for x in articles_coll:\n",
    "    body = x[2]\n",
    "    xw_kw_list = []\n",
    "    if len(body) > 0:\n",
    "        dict_entities = indicoio.keywords(body, top_n=15, version=2, relative=True, threshold=0.1)\n",
    "        for y in dict_entities.keys():\n",
    "            if not y in stopset:\n",
    "                xw_kw_list.append((y, \"keyword\"))\n",
    "        people_entities = indicoio.people(body, threshold=0.5)\n",
    "        for y in people_entities:\n",
    "            if not y['text'] in stopset:\n",
    "                people_tuple = y['text'], \"entity\"\n",
    "                xw_kw_list.append(people_tuple)\n",
    "        places_entities = indicoio.places(body, threshold=0.5)\n",
    "        for y in places_entities:\n",
    "            if not y['text'] in stopset:\n",
    "                places_tuple = y['text'], \"entity\"\n",
    "                xw_kw_list.append(places_tuple)\n",
    "        org_entities = indicoio.organizations(body, threshold=0.5)\n",
    "        for y in org_entities:\n",
    "            if not y['text'] in stopset:\n",
    "                org_tuple = y['text'], \"entity\"\n",
    "                xw_kw_list.append(org_tuple)\n",
    "    else:\n",
    "        pass\n",
    "    xw_kw_list.append(x[3].split(\",\"))\n",
    "    to_append = x[0], x[1], x[2], x[3], xw_kw_list\n",
    "    new_articles_coll.append(to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n"
     ]
    }
   ],
   "source": [
    "# index the webhose articles into an instance of elasticsearch to run queries against them\n",
    "\n",
    "i = 0\n",
    "for x in webhose_articles:\n",
    "    try:\n",
    "        es.index(index='xbwire5', doc_type='articles', id=i, body={\"article_title\": x[0], \"link\": x[1], \"content\": x[2], \"author\": x[3]})\n",
    "    except:\n",
    "        pass\n",
    "    i += 1\n",
    "    \n",
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using the keywords from the Indico.io algorithm, build queries to feed into elasticsearch, feed the queries back\n",
    "# a list with its elements to have them all travel together again\n",
    "\n",
    "articles_with_query = []\n",
    "\n",
    "for x in new_articles_coll:\n",
    "    queries = []\n",
    "    for y in x[4]:\n",
    "        if len(y) > 1:\n",
    "            word_type = y[1]\n",
    "            word = y[0]\n",
    "            if word_type == \"keyword\":\n",
    "                lower_title = x[0].lower()\n",
    "                formatted_keyword = word.encode('utf-8')\n",
    "                if word_type.find('\\n') > 0:\n",
    "                    pass\n",
    "                elif len(y) == 0:\n",
    "                    pass\n",
    "                elif formatted_keyword == \" \":\n",
    "                    pass\n",
    "                elif formatted_keyword.lower() in lower_title:\n",
    "                    title_word = '{\"multi_match\": ' + \"{\" + '\"query\": ' + '\"' + formatted_keyword + '\", ' + '\"type\": ' + '\"best_fields\", ' + '\"fields\": ' + '[ \"article_title^3\", \"content\" ], ' + '\"tie_breaker\": ' + \"0.3, \" + '\"minimum_should_match\": ' + '\"40%\"' +\"}}, \"\n",
    "                    queries.append(title_word)\n",
    "                elif \" \" in formatted_keyword:\n",
    "                    phrase = formatted_keyword.split(\" \")\n",
    "                    for new_word in phrase:\n",
    "                        if new_word in lower_title:\n",
    "                            partial_word = '{\"multi_match\": ' + \"{\" + '\"query\": ' + '\"' + new_word + '\", ' + '\"type\": ' + '\"best_fields\", ' + '\"fields\": ' + '[ \"article_title^3\", \"content\" ], ' + '\"tie_breaker\": ' + \"0.3, \" + '\"minimum_should_match\": ' + '\"40%\"' +\"}}, \"\n",
    "                            queries.append(partial_word)\n",
    "                else:\n",
    "                    try:\n",
    "                        word = '{\"multi_match\": ' + \"{\" + '\"query\": ' + '\"' + formatted_keyword + '\", ' + '\"type\": ' + '\"best_fields\", ' + '\"fields\": ' + '[ \"article_title\", \"content\" ], ' + '\"tie_breaker\": ' + \"0.3, \" + '\"minimum_should_match\": ' + '\"40%\"' +\"}}, \"\n",
    "                        queries.append(word)\n",
    "                    except:\n",
    "                        pass\n",
    "            elif word_type == \"entity\":\n",
    "                formatted_keyword = word.encode('utf-8')\n",
    "                if word.find('\\n') > 0:\n",
    "                    pass\n",
    "                elif len(y) == 0:\n",
    "                    pass\n",
    "                elif formatted_keyword == \" \":\n",
    "                    pass\n",
    "                elif formatted_keyword.lower() in lower_title:\n",
    "                    title_word = '{\"multi_match\": ' + \"{\" + '\"query\": ' + '\"' + formatted_keyword + '\", ' + '\"type\": ' + '\"best_fields\", ' + '\"fields\": ' + '[ \"article_title^3\", \"content^3\" ], ' + '\"tie_breaker\": ' + \"0.3, \" + '\"minimum_should_match\": ' + '\"30%\"' +\"}}, \"\n",
    "                    queries.append(title_word)\n",
    "                elif \" \" in formatted_keyword:\n",
    "                    phrase = formatted_keyword.split(\" \")\n",
    "                    for new_word in phrase:\n",
    "                        if new_word in lower_title:\n",
    "                            partial_word = '{\"multi_match\": ' + \"{\" + '\"query\": ' + '\"' + new_word + '\", ' + '\"type\": ' + '\"best_fields\", ' + '\"fields\": ' + '[ \"article_title^3\", \"content^3\" ], ' + '\"tie_breaker\": ' + \"0.3, \" + '\"minimum_should_match\": ' + '\"30%\"' +\"}}, \"\n",
    "                            queries.append(partial_word)\n",
    "                        else:\n",
    "                            pass\n",
    "                else:\n",
    "                    try:\n",
    "                        word = '{\"multi_match\": ' + \"{\" + '\"query\": ' + '\"' + formatted_keyword + '\", ' + '\"type\": ' + '\"best_fields\", ' + '\"fields\": ' + '[ \"article_title^2\", \"content^3\" ], ' + '\"tie_breaker\": ' + \"0.3, \" + '\"minimum_should_match\": ' + '\"30%\"' +\"}}, \"\n",
    "                        queries.append(word)\n",
    "                    except:\n",
    "                        pass\n",
    "    set_queries = set(queries)\n",
    "    htw = ''.join(set_queries)\n",
    "    length = len(htw)\n",
    "    format_htw = \"[\" + htw[:length-2] + \"]\"\n",
    "    to_add = [x[0], x[1], x[2], format_htw]\n",
    "    articles_with_query.append(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Article by article, query by query, gather results collected from elasticsearch and store them together\n",
    "# After that, use clustering and thresholds to combine the most relevant articles together that are most likely to be based off of\n",
    "# the Xbox Wire article\n",
    "\n",
    "Titles = []\n",
    "Article_Link = []\n",
    "Article_Relevance_Label = []\n",
    "Rlt_Article_Links = []    \n",
    "Article_Relevance_Score = []\n",
    "Article_Title =[]\n",
    "n = 1\n",
    "while n < 2:\n",
    "    for x in articles_with_query:\n",
    "        query = x[3]\n",
    "        if len(query) > 0:\n",
    "            searches = es.search(index=\"xbwire5\", body={\"query\": {\"bool\": {\"should\": ast.literal_eval(query)}}})\n",
    "            relevance_score = []\n",
    "            other_index = []\n",
    "            article_name = []\n",
    "            rlted_article_lk = []\n",
    "        for y in searches['hits']['hits']:\n",
    "            if y['_score'] < 1 and y['_score'] > 0.001:\n",
    "                relevance = y['_score']\n",
    "                relevance_score.append(relevance)\n",
    "                other_index.append(1)\n",
    "                article_name.append(y['_source']['article_title'])\n",
    "                rlted_article_lk.append(y['_source']['link'])\n",
    "            else:\n",
    "                pass\n",
    "        if len(relevance_score) > 3:\n",
    "            X = np.column_stack((relevance_score, other_index))\n",
    "            ms = MeanShift()\n",
    "            ms.fit(X)\n",
    "            labels = ms.labels_\n",
    "            cluster_centers = ms.cluster_centers_\n",
    "            n_clusters_ = len(np.unique(labels))\n",
    "            for i in range(len(X)):\n",
    "                if labels[i] != 0:\n",
    "                    Titles.append(x[0])\n",
    "                    Article_Link.append(x[1])\n",
    "                    Article_Title.append(article_name[i])\n",
    "                    Rlt_Article_Links.append(rlted_article_lk[i])\n",
    "                    Article_Relevance_Score.append(X[i])\n",
    "                    Article_Relevance_Label.append(labels[i])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use pandas to easily everything to a CSV document\n",
    "\n",
    "results = pd.DataFrame(columns=['Xbox Wire Title', 'Article Link', 'Related Article Title', 'Related Article Label', 'Related Article Link', 'Related Article Relevance'])\n",
    "\n",
    "results['Xbox Wire Title'] = Titles\n",
    "results['Article Link'] = Article_Link\n",
    "results['Related Article Title'] = Article_Title\n",
    "results['Related Article Link'] = Rlt_Article_Links\n",
    "results['Related Article Label'] = Article_Relevance_Label\n",
    "results['Related Article Relevance'] = Article_Relevance_Score\n",
    "\n",
    "results.to_csv(\"Article_Results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
